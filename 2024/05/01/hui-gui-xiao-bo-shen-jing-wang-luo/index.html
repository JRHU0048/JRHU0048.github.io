<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="小波神经网络回归小波神经网络（Wavelet Neural Network, WNN）是一种利用小波变换作为激活函数的前馈神经网络。小波变换是一种数学方法，用于将信号分解为构成其的小波，这些小波可以捕捉信号的局部特征。在神经网络中，小波变换可以作为一种强有力的工具来处理和分析数据，尤其是在时间序列分析和信号处理领域。 小波神经网络回归是指使用小波神经网络对数据进行回归分析，以预测连续的输出值。这种">
<meta property="og:type" content="article">
<meta property="og:title" content="回归———小波神经网络">
<meta property="og:url" content="http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/index.html">
<meta property="og:site_name" content="CHARMMY">
<meta property="og:description" content="小波神经网络回归小波神经网络（Wavelet Neural Network, WNN）是一种利用小波变换作为激活函数的前馈神经网络。小波变换是一种数学方法，用于将信号分解为构成其的小波，这些小波可以捕捉信号的局部特征。在神经网络中，小波变换可以作为一种强有力的工具来处理和分析数据，尤其是在时间序列分析和信号处理领域。 小波神经网络回归是指使用小波神经网络对数据进行回归分析，以预测连续的输出值。这种">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-05-01T06:31:00.000Z">
<meta property="article:modified_time" content="2024-05-01T07:12:38.977Z">
<meta property="article:author" content="JR.HU">
<meta name="twitter:card" content="summary">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>回归———小波神经网络</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
<meta name="generator" content="Hexo 7.1.1"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="目录"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="目录"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="顶部" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">首页</a></li><!--
     --><!--
       --><li><a href="/about/">关于</a></li><!--
     --><!--
       --><li><a href="/archives/">归档</a></li><!--
     --><!--
       --><li><a href="/tags/">标签</a></li><!--
     --><!--
       --><li><a href="/categories/">分类</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="上一篇" href="/2024/05/02/lun-wen-fu-xian-0/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="下一篇" href="/2024/05/01/yuan-bao-zi-dong-ji/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="返回顶部" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="分享文章" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/&text=回归———小波神经网络"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/&title=回归———小波神经网络"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/&is_video=false&description=回归———小波神经网络"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=回归———小波神经网络&body=Check out this article: http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/&title=回归———小波神经网络"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/&title=回归———小波神经网络"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/&title=回归———小波神经网络"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/&title=回归———小波神经网络"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/&name=回归———小波神经网络&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/&t=回归———小波神经网络"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E6%B3%A2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9B%9E%E5%BD%92"><span class="toc-number">1.</span> <span class="toc-text">小波神经网络回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B%E5%88%86%E6%9E%90"><span class="toc-number">2.</span> <span class="toc-text">实例分析</span></a></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        回归———小波神经网络
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">JR.HU</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2024-05-01T06:31:00.000Z" class="dt-published" itemprop="datePublished">2024-05-01</time>
        
      
    </div>


      

      

    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <h2 id="小波神经网络回归"><a href="#小波神经网络回归" class="headerlink" title="小波神经网络回归"></a>小波神经网络回归</h2><p>小波神经网络（Wavelet Neural Network, WNN）是一种利用小波变换作为激活函数的前馈神经网络。小波变换是一种数学方法，用于将信号分解为构成其的小波，这些小波可以捕捉信号的局部特征。在神经网络中，小波变换可以作为一种强有力的工具来处理和分析数据，尤其是在时间序列分析和信号处理领域。</p>
<p>小波神经网络回归是指使用小波神经网络对数据进行回归分析，以预测连续的输出值。这种回归分析可以应用于多种领域，比如金融市场预测、气象预测、交通流量预测等。</p>
<p>小波神经网络的关键特点包括：</p>
<ol>
<li><p><strong>小波变换</strong>：用作隐含层的激活函数，能够捕捉数据的局部特征。</p>
</li>
<li><p><strong>自适应学习</strong>：网络通过学习数据集来自动调整其权值和阈值。</p>
</li>
<li><p><strong>非线性映射</strong>：小波变换提供了一种从输入空间到高维特征空间的非线性映射，有助于网络捕捉复杂的非线性关系。</p>
</li>
<li><p><strong>泛化能力</strong>：通过训练，小波神经网络能够学习到数据的内在规律，从而对未知数据进行有效的预测。</p>
</li>
<li><p><strong>优化算法</strong>：通常使用梯度下降或其变种（如动量法、AdaGrad等）来优化网络的权值，以最小化预测误差。</p>
</li>
</ol>
<p>在小波神经网络回归中，通常的步骤包括：</p>
<ul>
<li><p><strong>数据预处理</strong>：包括归一化处理，使数据处于一个合适的范围内，提高网络训练的稳定性和收敛速度。</p>
</li>
<li><p><strong>网络初始化</strong>：随机初始化网络的权值和阈值。</p>
</li>
<li><p><strong>前向传播</strong>：计算隐含层的小波变换，并将结果传递到输出层。</p>
</li>
<li><p><strong>计算误差</strong>：使用某种误差度量（如均方误差）计算网络预测值和实际值之间的差异。</p>
</li>
<li><p><strong>反向传播</strong>：根据误差计算每个参数的梯度，并更新权值以减少误差。</p>
</li>
<li><p><strong>迭代优化</strong>：重复前向传播和反向传播步骤，直到网络达到预定的迭代次数或误差降低到可接受的程度。</p>
</li>
<li><p><strong>模型评估</strong>：使用测试数据集评估网络的预测性能。</p>
</li>
<li><p><strong>结果分析</strong>：对模型的预测结果进行分析，以了解其准确性和可靠性。</p>
</li>
</ul>
<p>小波神经网络因其在处理具有局部特征的信号方面的优势而被广泛应用于回归分析中。然而，选择合适的小波基和网络结构对于获得良好的预测性能至关重要。</p>
<h2 id="实例分析"><a href="#实例分析" class="headerlink" title="实例分析"></a>实例分析</h2><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br></pre></td><td class="code"><pre><span class="line">clc; <span class="comment">% 清除命令窗口</span></span><br><span class="line">clear; <span class="comment">% 清除所有变量</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% 网络参数配置</span></span><br><span class="line">load traffic_flux input output input_test output_test; <span class="comment">% 加载训练数据和测试数据</span></span><br><span class="line"></span><br><span class="line">M<span class="built_in">=</span>size(input,<span class="number">2</span>); <span class="comment">% 输入节点个数</span></span><br><span class="line">N<span class="built_in">=</span>size(output,<span class="number">2</span>); <span class="comment">% 输出节点个数</span></span><br><span class="line"></span><br><span class="line">n<span class="built_in">=</span><span class="number">6</span>; <span class="comment">% 隐含层节点个数</span></span><br><span class="line">lr1<span class="built_in">=</span><span class="number">0.01</span>; <span class="comment">% 学习率1</span></span><br><span class="line">lr2<span class="built_in">=</span><span class="number">0.001</span>; <span class="comment">% 学习率2</span></span><br><span class="line">maxgen<span class="built_in">=</span><span class="number">100</span>; <span class="comment">% 迭代次数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 权值初始化</span></span><br><span class="line">Wjk<span class="built_in">=</span>randn(n,M); Wjk_1<span class="built_in">=</span>Wjk; Wjk_2<span class="built_in">=</span>Wjk_1; <span class="comment">% 隐含层到输入层的权值</span></span><br><span class="line">Wij<span class="built_in">=</span>randn(N,n); Wij_1<span class="built_in">=</span>Wij; Wij_2<span class="built_in">=</span>Wij_1; <span class="comment">% 输出层到隐含层的权值</span></span><br><span class="line">a<span class="built_in">=</span>randn(<span class="number">1</span>,n); a_1<span class="built_in">=</span>a; a_2<span class="built_in">=</span>a_1; <span class="comment">% 激活函数的参数a</span></span><br><span class="line">b<span class="built_in">=</span>randn(<span class="number">1</span>,n); b_1<span class="built_in">=</span>b; b_2<span class="built_in">=</span>b_1; <span class="comment">% 激活函数的参数b</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 节点初始化</span></span><br><span class="line">y<span class="built_in">=</span>zeros(<span class="number">1</span>,N); <span class="comment">% 输出层的初始化</span></span><br><span class="line">net<span class="built_in">=</span>zeros(<span class="number">1</span>,n); <span class="comment">% 隐含层的初始化</span></span><br><span class="line">net_ab<span class="built_in">=</span>zeros(<span class="number">1</span>,n); <span class="comment">% 隐含层激活函数的输入初始化</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 权值学习增量初始化</span></span><br><span class="line">d_Wjk<span class="built_in">=</span>zeros(n,M); <span class="comment">% Wjk的梯度</span></span><br><span class="line">d_Wij<span class="built_in">=</span>zeros(N,n); <span class="comment">% Wij的梯度</span></span><br><span class="line">d_a<span class="built_in">=</span>zeros(<span class="number">1</span>,n); <span class="comment">% a的梯度</span></span><br><span class="line">d_b<span class="built_in">=</span>zeros(<span class="number">1</span>,n); <span class="comment">% b的梯度</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% 输入输出数据归一化</span></span><br><span class="line">[inputn,inputps]<span class="built_in">=</span>mapminmax(input<span class="string">&#x27;); % 输入数据归一化并保存参数</span></span><br><span class="line"><span class="string">[outputn,outputps]=mapminmax(output&#x27;</span>); <span class="comment">% 输出数据归一化并保存参数</span></span><br><span class="line">inputn<span class="built_in">=</span>inputn<span class="string">&#x27;; % 转置</span></span><br><span class="line"><span class="string">outputn=outputn&#x27;</span>; <span class="comment">% 转置</span></span><br><span class="line"></span><br><span class="line">error<span class="built_in">=</span>zeros(<span class="number">1</span>,maxgen); <span class="comment">% 初始化误差数组</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% 网络训练</span></span><br><span class="line">for i<span class="built_in">=</span><span class="number">1</span>:maxgen <span class="comment">% 迭代次数循环</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">% 误差累计</span></span><br><span class="line">    error(i)<span class="built_in">=</span><span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">% 循环训练</span></span><br><span class="line">    for kk<span class="built_in">=</span><span class="number">1</span>:size(input,<span class="number">1</span>) <span class="comment">% 对每个输入样本进行训练</span></span><br><span class="line">        x<span class="built_in">=</span>inputn(kk,:); <span class="comment">% 当前输入样本</span></span><br><span class="line">        yqw<span class="built_in">=</span>outputn(kk,:); <span class="comment">% 当前输出样本</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">% 计算隐含层的输出</span></span><br><span class="line">        for j<span class="built_in">=</span><span class="number">1</span>:n</span><br><span class="line">            for k<span class="built_in">=</span><span class="number">1</span>:M</span><br><span class="line">                net(j)<span class="built_in">=</span>net(j)+Wjk(j,k)*x(k); <span class="comment">% 计算隐含层的输入加权和</span></span><br><span class="line">            end</span><br><span class="line">            net_ab(j)<span class="built_in">=</span>(net(j)-b(j))/a(j); <span class="comment">% 应用小波变换</span></span><br><span class="line">            temp<span class="built_in">=</span>mymorlet(net_ab(j)); <span class="comment">% 计算小波函数的值</span></span><br><span class="line">            for k<span class="built_in">=</span><span class="number">1</span>:N</span><br><span class="line">                y<span class="built_in">=</span>y+Wij(k,j)*temp; <span class="comment">% 计算输出层的输出</span></span><br><span class="line">            end</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        <span class="comment">% 计算误差和</span></span><br><span class="line">        error(i)<span class="built_in">=</span>error(i)+sum(abs(yqw-y)); <span class="comment">% 计算当前样本的误差</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">% 权值调整</span></span><br><span class="line">        for j<span class="built_in">=</span><span class="number">1</span>:n</span><br><span class="line">            <span class="comment">% 计算d_Wij</span></span><br><span class="line">            temp<span class="built_in">=</span>mymorlet(net_ab(j));</span><br><span class="line">            for k<span class="built_in">=</span><span class="number">1</span>:N</span><br><span class="line">                d_Wij(k,j)<span class="built_in">=</span>d_Wij(k,j)-(yqw(k)-y(k))*temp; <span class="comment">% 计算Wij的梯度</span></span><br><span class="line">            end</span><br><span class="line">            <span class="comment">% 计算d_Wjk</span></span><br><span class="line">            temp<span class="built_in">=</span>d_mymorlet(net_ab(j)); <span class="comment">% 计算小波函数的导数</span></span><br><span class="line">            for k<span class="built_in">=</span><span class="number">1</span>:M</span><br><span class="line">                for l<span class="built_in">=</span><span class="number">1</span>:N</span><br><span class="line">                    d_Wjk(j,k)<span class="built_in">=</span>d_Wjk(j,k)+(yqw(l)-y(l))*Wij(l,j); <span class="comment">% 计算Wjk的梯度</span></span><br><span class="line">                end</span><br><span class="line">                d_Wjk(j,k)<span class="built_in">=</span>-d_Wjk(j,k)*temp*x(k)/a(j); <span class="comment">% 更新Wjk的梯度</span></span><br><span class="line">            end</span><br><span class="line">            <span class="comment">% 计算d_b</span></span><br><span class="line">            for k<span class="built_in">=</span><span class="number">1</span>:N</span><br><span class="line">                d_b(j)<span class="built_in">=</span>d_b(j)+(yqw(k)-y(k))*Wij(k,j); <span class="comment">% 计算b的梯度</span></span><br><span class="line">            end</span><br><span class="line">            d_b(j)<span class="built_in">=</span>d_b(j)*temp/a(j); <span class="comment">% 更新b的梯度</span></span><br><span class="line">            <span class="comment">% 计算d_a</span></span><br><span class="line">            for k<span class="built_in">=</span><span class="number">1</span>:N</span><br><span class="line">                d_a(j)<span class="built_in">=</span>d_a(j)+(yqw(k)-y(k))*Wij(k,j); <span class="comment">% 计算a的梯度</span></span><br><span class="line">            end</span><br><span class="line">            d_a(j)<span class="built_in">=</span>d_a(j)*temp*((net(j)-b(j))/b(j))/a(j); <span class="comment">% 更新a的梯度</span></span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        <span class="comment">% 权值参数更新      </span></span><br><span class="line">        Wij<span class="built_in">=</span>Wij-lr1*d_Wij; <span class="comment">% 更新Wij</span></span><br><span class="line">        Wjk<span class="built_in">=</span>Wjk-lr1*d_Wjk; <span class="comment">% 更新Wjk</span></span><br><span class="line">        b<span class="built_in">=</span>b-lr2*d_b; <span class="comment">% 更新b</span></span><br><span class="line">        a<span class="built_in">=</span>a-lr2*d_a; <span class="comment">% 更新a</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment">% 重置梯度和节点状态</span></span><br><span class="line">        d_Wjk<span class="built_in">=</span>zeros(n,M);</span><br><span class="line">        d_Wij<span class="built_in">=</span>zeros(N,n);</span><br><span class="line">        d_a<span class="built_in">=</span>zeros(<span class="number">1</span>,n);</span><br><span class="line">        d_b<span class="built_in">=</span>zeros(<span class="number">1</span>,n);</span><br><span class="line">        </span><br><span class="line">        y<span class="built_in">=</span>zeros(<span class="number">1</span>,N);</span><br><span class="line">        net<span class="built_in">=</span>zeros(<span class="number">1</span>,n);</span><br><span class="line">        net_ab<span class="built_in">=</span>zeros(<span class="number">1</span>,n);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">% 存储旧的权值和参数用于后续更新</span></span><br><span class="line">        Wjk_1<span class="built_in">=</span>Wjk; Wjk_2<span class="built_in">=</span>Wjk_1;</span><br><span class="line">        Wij_1<span class="built_in">=</span>Wij; Wij_2<span class="built_in">=</span>Wij_1;</span><br><span class="line">        a_1<span class="built_in">=</span>a; a_2<span class="built_in">=</span>a_1;</span><br><span class="line">        b_1<span class="built_in">=</span>b; b_2<span class="built_in">=</span>b_1;</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line"><span class="comment">%% 网络预测</span></span><br><span class="line"><span class="comment">% 预测输入归一化</span></span><br><span class="line">x<span class="built_in">=</span>mapminmax(<span class="string">&#x27;apply&#x27;</span>,input_test<span class="string">&#x27;,inputps); % 应用归一化参数</span></span><br><span class="line"><span class="string">x=x&#x27;</span>; <span class="comment">% 转置</span></span><br><span class="line">yuce<span class="built_in">=</span>zeros(<span class="number">92</span>,<span class="number">1</span>); <span class="comment">% 初始化预测输出数组</span></span><br><span class="line"><span class="comment">% 网络预测</span></span><br><span class="line">for i<span class="built_in">=</span><span class="number">1</span>:<span class="number">92</span> <span class="comment">% 对每个测试样本进行预测</span></span><br><span class="line">    x_test<span class="built_in">=</span>x(i,:); <span class="comment">% 当前测试输入样本</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">% 计算隐含层的输出</span></span><br><span class="line">    for j<span class="built_in">=</span><span class="number">1</span>:<span class="number">1</span>:n</span><br><span class="line">        for k<span class="built_in">=</span><span class="number">1</span>:<span class="number">1</span>:M</span><br><span class="line">            net(j)<span class="built_in">=</span>net(j)+Wjk(j,k)*x_test(k); <span class="comment">% 计算隐含层的输入加权和</span></span><br><span class="line">            net_ab(j)<span class="built_in">=</span>(net(j)-b(j))/a(j); <span class="comment">% 应用小波变换</span></span><br><span class="line">        end</span><br><span class="line">        temp<span class="built_in">=</span>mymorlet(net_ab(j)); <span class="comment">% 计算小波函数的值</span></span><br><span class="line">        for k<span class="built_in">=</span><span class="number">1</span>:N</span><br><span class="line">            y(k)<span class="built_in">=</span>y(k)+Wij(k,j)*temp; <span class="comment">% 计算输出层的输出</span></span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    yuce(i)<span class="built_in">=</span>y(k); <span class="comment">% 存储预测结果</span></span><br><span class="line">    <span class="comment">% 重置节点状态为下一个样本做准备</span></span><br><span class="line">    y<span class="built_in">=</span>zeros(<span class="number">1</span>,N);</span><br><span class="line">    net<span class="built_in">=</span>zeros(<span class="number">1</span>,n);</span><br><span class="line">    net_ab<span class="built_in">=</span>zeros(<span class="number">1</span>,n);</span><br><span class="line">end</span><br><span class="line"><span class="comment">% 预测输出反归一化</span></span><br><span class="line">ynn<span class="built_in">=</span>mapminmax(<span class="string">&#x27;reverse&#x27;</span>,yuce,outputps); <span class="comment">% 应用反归一化参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% 结果分析</span></span><br><span class="line">figure(<span class="number">1</span>); <span class="comment">% 创建图形窗口</span></span><br><span class="line">plot(ynn,<span class="string">&#x27;r*:&#x27;</span>); <span class="comment">% 绘制预测的交通流量</span></span><br><span class="line">hold on; <span class="comment">% 保持当前图形</span></span><br><span class="line">plot(output_test,<span class="string">&#x27;bo--&#x27;</span>); <span class="comment">% 绘制实际的交通流量</span></span><br><span class="line">title(<span class="string">&#x27;预测交通流量&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">12</span>); <span class="comment">% 设置图形标题</span></span><br><span class="line">legend(<span class="string">&#x27;预测交通流量&#x27;</span>,<span class="string">&#x27;实际交通流量&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">12</span>); <span class="comment">% 添加图例</span></span><br><span class="line">xlabel(<span class="string">&#x27;时间点&#x27;</span>); <span class="comment">% 设置x轴标签</span></span><br><span class="line">ylabel(<span class="string">&#x27;交通流量&#x27;</span>); <span class="comment">% 设置y轴标签</span></span><br></pre></td></tr></table></figure>

<p>这里用到的两个子程序分别是：</p>
<figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">function y<span class="built_in">=</span>mymorlet(t)</span><br><span class="line"></span><br><span class="line">y <span class="built_in">=</span> exp(-(t.^<span class="number">2</span>)/<span class="number">2</span>) * cos(<span class="number">1.75</span>*t);</span><br></pre></td></tr></table></figure>
<p>函数 <code>mymorlet</code> 是一个自定义的函数，它定义了一个莫勒特小波（Morlet wavelet），也称为Gabor小波。莫勒特小波是一种复数小波，常用于信号处理和小波变换中，尤其是在连续小波变换（Continuous Wavelet Transform, CWT）中。</p>
<p>函数的功能是计算输入参数 <code>t</code> 对应的莫勒特小波的值。莫勒特小波是一个复数函数，由高斯函数的指数部分和一个余弦函数相乘组成。具体来说，莫勒特小波可以表示为：</p>
<p>$\psi(t) &#x3D; e^{-\frac{t^2}{2}} \cdot \cos(1.75 \cdot t)$</p>
<p>其中：</p>
<ul>
<li>$e^{-\frac{t^2}{2}}$  是高斯包络，它控制着小波在时间域的宽度。</li>
<li>$\cos(1.75 \cdot t)$  是小波的振荡部分，它控制着小波的频率。</li>
</ul>
<p>莫勒特小波由于其良好的局部化特性，常用于分析具有振荡性质的非平稳信号，例如在脑电波（EEG）分析、地震信号分析等领域。在小波神经网络中，莫勒特小波可以作为激活函数，帮助网络捕捉输入数据的局部特征。</p>
<figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">function y<span class="built_in">=</span>d_mymorlet(t)</span><br><span class="line"></span><br><span class="line">y <span class="built_in">=</span> -<span class="number">1.75</span>*sin(<span class="number">1.75</span>*t).*exp(-(t.^<span class="number">2</span>)/<span class="number">2</span>)-t* cos(<span class="number">1.75</span>*t).*exp(-(t.^<span class="number">2</span>)/<span class="number">2</span>) ;</span><br></pre></td></tr></table></figure>
<p> <code>d_mymorlet</code> 定义了莫勒特小波（Morlet wavelet）的导数。这个函数计算这个小波在时间 <code>t</code> 处的导数，即小波变换中的时间导数。</p>
<p>函数的功能是计算输入参数 <code>t</code> 对应的莫勒特小波导数的值。这个导数由两部分组成：</p>
<ol>
<li><p>$-1.75 \sin(1.75 \cdot t) \cdot e^{-\frac{t^2}{2}}$ ：这是莫勒特小波中余弦项的导数，由链式法则和余弦函数的导数（负的正弦函数）得到。</p>
</li>
<li><p>$-t \cdot \cos(1.75 \cdot t) \cdot e^{-\frac{t^2}{2}}$：这是莫勒特小波中高斯项与余弦项乘积的导数，由高斯项 <code>t</code> 的导数（1）和乘积的导数构成。</p>
</li>
</ol>
<p>莫勒特小波的导数在小波变换中非常重要，因为它提供了信号局部变化率的信息。在小波神经网络中，这个导数可能用于捕捉输入数据的局部变化特征，从而提高网络对信号变化的敏感性。</p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">首页</a></li>
        
          <li><a href="/about/">关于</a></li>
        
          <li><a href="/archives/">归档</a></li>
        
          <li><a href="/tags/">标签</a></li>
        
          <li><a href="/categories/">分类</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E6%B3%A2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9B%9E%E5%BD%92"><span class="toc-number">1.</span> <span class="toc-text">小波神经网络回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B%E5%88%86%E6%9E%90"><span class="toc-number">2.</span> <span class="toc-text">实例分析</span></a></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/&text=回归———小波神经网络"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/&title=回归———小波神经网络"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/&is_video=false&description=回归———小波神经网络"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=回归———小波神经网络&body=Check out this article: http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/&title=回归———小波神经网络"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/&title=回归———小波神经网络"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/&title=回归———小波神经网络"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/&title=回归———小波神经网络"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/&name=回归———小波神经网络&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://jrhu0048.github.io/2024/05/01/hui-gui-xiao-bo-shen-jing-wang-luo/&t=回归———小波神经网络"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2022-2024
    JR.HU
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">首页</a></li><!--
     --><!--
       --><li><a href="/about/">关于</a></li><!--
     --><!--
       --><li><a href="/archives/">归档</a></li><!--
     --><!--
       --><li><a href="/tags/">标签</a></li><!--
     --><!--
       --><li><a href="/categories/">分类</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"复制到粘贴板！\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "复制成功！");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
